{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 : Explain one hot encoding "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS :one hot encoding is a technique used in ML and MLP to encode categorical data into numeircal format .We can do it using scikit-learn library\n",
    "\n",
    "Example : suppose color are Red,Blue and Green are of category now we want to encode them .Before encoding color must be specifying any of type like Red-->1 ,Blue ->2,Green ->3 then we have to convert into binary vectore like one represeting 1 them other are 0 \n",
    "\n",
    "so \n",
    "Red->1 then it will be [1,0,0]\n",
    "Blue ->1 then it will be [0,1,0]\n",
    "and finally Green ->1 then it will be [0,0,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 :Explain Bag of Words:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bag of Words (BoW) is a common technique used in natural language processing (NLP) and text analysis. It represents a document as an unordered set of words, disregarding grammar and word order but keeping track of the frequency of each word. Each word in the document is considered as a \"feature,\" and the resulting representation is a \"bag\" of these features.\n",
    "\n",
    "Advantages of the Bag of Words model:\n",
    "\n",
    "Simplicity: It's a simple and intuitive representation that is easy to understand and implement.\n",
    "\n",
    "Versatility: It can be used for various NLP tasks such as text classification, sentiment analysis, and clustering.\n",
    "\n",
    "Efficiency: It's computationally efficient and works well with large datasets.\n",
    "\n",
    "\n",
    "Disadvantages of the Bag of Words model:\n",
    "\n",
    "Loss of Sequence Information: It doesn't consider the order of words, which means it loses the sequence information present in the text.\n",
    "High Dimensionality: The resulting feature vectors can be very high-dimensional, especially with a large vocabulary, leading to a sparse matrix that can be memory-intensive.\n",
    "Ignoring Semantic Relationships: It doesn't capture the semantic relationships between words.\n",
    "Equal Importance of Words: All words are treated equally, regardless of their importance or meaning.\n",
    "\n",
    "In practice, more advanced techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings (e.g., Word2Vec, GloVe) are often used to address some of the limitations of the Bag of Words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents into a bag of words\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array for better visibility\n",
    "dense_array = X.toarray()\n",
    "\n",
    "# Display the bag of words as a DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dense_array, columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 3 :Explain Bag of N-Grams ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS :A bag of n-grams is a common feature representation used in natural language processing (NLP) and text analysis. It is a way of representing text data by capturing the frequency of different n-grams, where an n-gram is a contiguous sequence of n items (usually words) from a given sample of text or speech.\n",
    "\n",
    "Here's a breakdown of the concept:\n",
    "\n",
    "1. **Tokenization:** Before creating a bag of n-grams, the text is typically tokenized, which means breaking it down into smaller units, such as words.\n",
    "\n",
    "2. **N-grams:** An n-gram is a sequence of n items. In the context of text analysis, these items are usually words. For example:\n",
    "   - Unigram (1-gram): \"cat\"\n",
    "   - Bigram (2-gram): \"the cat\", \"cat sat\"\n",
    "   - Trigram (3-gram): \"the cat sat\", \"cat sat on\"\n",
    "   - and so on.\n",
    "\n",
    "3. **Counting Frequencies:** The bag of n-grams representation then involves counting the frequency of each n-gram in the text. This can be done using a simple count vectorizer.\n",
    "\n",
    "4. **Bag of N-grams:** The result is a \"bag\" (or a vector) of n-grams, where each element represents the count or frequency of a particular n-gram in the text.\n",
    "\n",
    "For example, given the text \"The cat sat on the mat,\" the bag of bigrams would be:\n",
    "- \"the cat\": 1\n",
    "- \"cat sat\": 1\n",
    "- \"sat on\": 1\n",
    "- \"on the\": 1\n",
    "- \"the mat\": 1\n",
    "\n",
    "The order of the words is not considered in a traditional bag of n-grams representation, which is why it's referred to as a \"bag\" — it only captures the occurrence of n-grams, not their sequence.\n",
    "\n",
    "This approach is commonly used in tasks such as text classification, sentiment analysis, and information retrieval, where understanding the distribution of words or phrases in a document is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data\n",
    "corpus = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked at the cat.\",\n",
    "    \"The cat chased the mouse.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer with n-gram range (1, 2) for unigrams and bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (n-grams)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array for easier inspection\n",
    "dense_array = X.toarray()\n",
    "\n",
    "# Display the results\n",
    "print(\"Names (N-grams):\", feature_names)\n",
    "print(\"Bag of N-grams:\")\n",
    "print(dense_array)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 4: Explain TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:TF-IDF stands for Term Frequency-Inverse Document Frequency, and it is a numerical statistic used in information retrieval and text analysis. TF-IDF is used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It is commonly used in natural language processing and text mining.\n",
    "\n",
    "Here's a brief explanation of the two components:\n",
    "\n",
    "1. **Term Frequency (TF):**\n",
    "   - This measures how often a term (word) appears in a document.\n",
    "   - It is calculated as the number of times a term occurs in a document divided by the total number of terms in that document.\n",
    "\n",
    "   \\[ \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} \\]\n",
    "\n",
    "2. **Inverse Document Frequency (IDF):**\n",
    "   - This measures how important a term is across a collection of documents.\n",
    "   - It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term.\n",
    "\n",
    "   \\[ \\text{IDF}(t, D) = \\log\\left(\\frac{\\text{Total number of documents in the corpus } N}{\\text{Number of documents containing term } t + 1}\\right) \\]\n",
    "\n",
    "The overall TF-IDF score for a term \\( t \\) in a document \\( d \\) is then calculated as the product of TF and IDF:\n",
    "\n",
    "\\[ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) \\]\n",
    "\n",
    "The idea behind TF-IDF is that it highlights terms that are frequent in a document but not common across all documents, thus giving more weight to terms that are potentially more informative or indicative of the content of a document. This technique is widely used in information retrieval, document classification, and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents =\"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n",
    "            \n",
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps =PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "documents=nltk.sent_tokenize(documents)\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words) and their indices\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array\n",
    "dense_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Create a dictionary to store TF-IDF scores for each word in each document\n",
    "tfidf_scores = {}\n",
    "for i, doc in enumerate(documents):\n",
    "    tfidf_scores[f\"Document {i + 1}\"] = {word: score for word, score in zip(feature_names, dense_array[i])}\n",
    "\n",
    "# Print the TF-IDF scores for each word in each document\n",
    "for doc, scores in tfidf_scores.items():\n",
    "    print(f\"{doc}:\")\n",
    "    for word, score in scores.items():\n",
    "        print(f\"{word}: {score:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch ineuron</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ineuron watch ineuron</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comments</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ineuron write comments</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text  output\n",
       "0    people watch ineuron       1\n",
       "1  ineuron watch ineuron        1\n",
       "2   people write comments       0\n",
       "3  ineuron write comments       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'text':['people watch ineuron','ineuron watch ineuron ','people write comments','ineuron write comments'],'output':[1,1,0,0]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.49681612, 0.61366674, 0.61366674, 0.        ],\n",
       "       [0.        , 0.8508161 , 0.        , 0.52546357, 0.        ],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027],\n",
       "       [0.61366674, 0.49681612, 0.        , 0.        , 0.61366674]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid.fit_transform(df['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.51082562 1.22314355 1.51082562 1.51082562 1.51082562]\n",
      "['comments' 'ineuron' 'people' 'watch' 'write']\n"
     ]
    }
   ],
   "source": [
    "print(tfid.idf_)\n",
    "print(tfid.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 5: What is OOV?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS :OOV stands for \"Out of Vocabulary\" or \"Out of Vocabulary Words\" in the context of natural language processing (NLP) and text analysis. It refers to words that are not present in the vocabulary or word list of a particular language model or system.\n",
    "\n",
    "In NLP tasks such as machine translation, text classification, or sentiment analysis, a model is trained on a specific vocabulary, which includes a set of words it has seen during training. When the model encounters a word during testing or real-world use that is not present in its training vocabulary, it considers it as an out-of-vocabulary word or OOV.\n",
    "\n",
    "Handling OOV words is important for ensuring the robustness and generalization of language models. Several strategies can be employed to deal with OOV words:\n",
    "\n",
    "1. **Unknown Token:** Replace OOV words with a special token (often `<UNK>` or `<OOV>`) to indicate that the word is unknown to the model.\n",
    "\n",
    "2. **Subword Tokenization:** Use subword tokenization techniques, such as Byte Pair Encoding (BPE) or SentencePiece, to tokenize words into smaller subword units. This helps models handle unseen words by composing them from known subword units.\n",
    "\n",
    "3. **Word Embeddings:** If using word embeddings, pre-trained embeddings (e.g., Word2Vec, GloVe, FastText) might capture semantic information about OOV words based on their similarity to known words. However, this approach is limited to embeddings learned during training.\n",
    "\n",
    "4. **Dynamic Vocabulary:** In some cases, models allow for dynamic expansion of their vocabulary during testing or deployment to accommodate previously unseen words.\n",
    "\n",
    "Handling OOV words is particularly crucial in real-world applications where the model might encounter new words, names, or terms that were not present in the training data. Effective OOV handling contributes to the model's ability to generalize and perform well on diverse datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 6 What are word embedings?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS :Word embedding is a technique in natural language processing (NLP) that represents words as vectors in a continuous vector space. The goal of word embedding is to capture semantic relationships between words, allowing machines to understand the meaning of words and their contextual associations. This representation is essential for various NLP tasks, such as machine translation, sentiment analysis, and named entity recognition.\n",
    "\n",
    "There are several types of word embedding techniques, and here are some of the most commonly used ones:\n",
    "\n",
    "1. **Count-Based Methods:**\n",
    "   - **Term Frequency-Inverse Document Frequency (TF-IDF):** This method represents words based on their frequency in a document and their rarity in a corpus. However, TF-IDF does not capture semantic relationships between words.\n",
    "\n",
    "   - **Count Vectorization:** It represents words as vectors based on their frequency in a document. Each word is assigned a unique index, and the vectors contain the count of each word in the document.\n",
    "\n",
    "2. **Predictive Models:**\n",
    "   - **Word2Vec (Word to Vector):** Developed by Google, Word2Vec learns word embeddings by predicting a word based on its context (Continuous Bag of Words - CBOW) or predicting the context given a word (Skip-gram). Word2Vec embeddings are known for capturing semantic relationships well.\n",
    "\n",
    "   - **GloVe (Global Vectors for Word Representation):** GloVe is based on the global statistics of the corpus and focuses on co-occurrence probabilities. It considers the overall word co-occurrence in the entire dataset to create word embeddings.\n",
    "\n",
    "3. **Neural Embeddings:**\n",
    "   - **FastText:** Developed by Facebook, FastText extends Word2Vec by representing words as bags of character n-grams. It is particularly useful for handling out-of-vocabulary words and capturing morphological information.\n",
    "\n",
    "   - **BERT (Bidirectional Encoder Representations from Transformers):** BERT is a transformer-based model that considers the context from both directions (left and right) during training. It has significantly improved the state-of-the-art in various NLP tasks.\n",
    "\n",
    "   - **ELMo (Embeddings from Language Models):** ELMo generates word embeddings by considering the context and capturing polysemy (multiple meanings of a word) through deep contextualized representations.\n",
    "\n",
    "These word embedding techniques allow machines to understand the relationships and meanings of words in a more nuanced way, enabling improved performance in a wide range of natural language processing tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7.\tExplain Continuous bag of words (CBOW)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANs:CBOW, which stands for Continuous Bag of Words, is a model used in natural language processing (NLP) and is particularly popular for word embedding tasks. Word embeddings are dense vector representations of words that capture semantic relationships between words. CBOW is one of the architectures used to generate these word embeddings.\n",
    "\n",
    "Here's a brief explanation of how CBOW works:\n",
    "\n",
    "1. **Context and Target Words:**\n",
    "   - CBOW doesn't predict words based on their surrounding context; instead, it predicts a target word based on its context.\n",
    "   - The model takes a fixed-sized context window of surrounding words (context words) as input and aims to predict the target word.\n",
    "\n",
    "2. **Neural Network Architecture:**\n",
    "   - CBOW uses a neural network with a single hidden layer.\n",
    "   - The input layer has nodes representing the one-hot encoded vectors of context words. Each node corresponds to a unique word in the vocabulary, and its value is either 0 or 1, indicating the presence or absence of the word in the context window.\n",
    "   - The hidden layer has fewer nodes than the input layer, and it transforms the one-hot encoded input into a dense vector representation.\n",
    "\n",
    "3. **Objective Function:**\n",
    "   - The objective of CBOW is to minimize the prediction error of the target word given the context.\n",
    "   - The output layer produces a probability distribution over the entire vocabulary, and the model is trained using a softmax activation function to predict the target word.\n",
    "\n",
    "4. **Training:**\n",
    "   - The model is trained using a dataset containing pairs of context and target words.\n",
    "   - During training, the weights of the neural network are adjusted to minimize the difference between the predicted probabilities and the actual distribution of the target words.\n",
    "\n",
    "5. **Word Embeddings:**\n",
    "   - Once the model is trained, the weights of the hidden layer represent the word embeddings. These embeddings capture semantic relationships between words based on their co-occurrence patterns in the training data.\n",
    "\n",
    "CBOW is efficient and tends to perform well on tasks where the meaning of a word is largely determined by its context. However, it may not capture word order information as effectively as other models like Skip-Gram, another popular word embedding model. Skip-Gram predicts the context words given a target word and is sometimes preferred when word order is crucial for the task at hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 8.\tExplain SkipGram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS :Skip-Gram is another word embedding model used in natural language processing (NLP). Like CBOW (Continuous Bag of Words), Skip-Gram is designed to generate word embeddings, which are dense vector representations of words that capture semantic relationships between them. The primary difference between Skip-Gram and CBOW lies in their objectives during training.\n",
    "\n",
    "Here's an overview of how Skip-Gram works:\n",
    "\n",
    "1. **Objective:**\n",
    "   - Skip-Gram aims to predict the context words (surrounding words) given a target word.\n",
    "   - Unlike CBOW, which predicts a target word based on its context, Skip-Gram flips the task and predicts the context words based on a target word.\n",
    "\n",
    "2. **Neural Network Architecture:**\n",
    "   - Skip-Gram also uses a neural network with a single hidden layer.\n",
    "   - The input layer represents the one-hot encoded vector of the target word.\n",
    "   - The hidden layer transforms the one-hot encoded input into a dense vector representation.\n",
    "   - The output layer has nodes corresponding to the entire vocabulary, and it uses a softmax activation function to predict the probability distribution of context words.\n",
    "\n",
    "3. **Training:**\n",
    "   - During training, the model is presented with pairs of target words and their context words.\n",
    "   - The objective is to adjust the weights of the neural network to maximize the probability of predicting the correct context words given a target word.\n",
    "   - The training process involves updating the weights using optimization algorithms (such as stochastic gradient descent) to minimize the difference between predicted and actual context word probabilities.\n",
    "\n",
    "4. **Word Embeddings:**\n",
    "   - Similar to CBOW, once the Skip-Gram model is trained, the weights of the hidden layer represent the word embeddings.\n",
    "   - These embeddings capture semantic relationships between words based on their co-occurrence patterns in the training data.\n",
    "\n",
    "5. **Performance and Use Cases:**\n",
    "   - Skip-Gram tends to perform well when capturing semantic relationships between words, especially in cases where word order is crucial.\n",
    "   - It is often preferred in tasks where the meaning of a word is strongly influenced by its surrounding context.\n",
    "\n",
    "In summary, while CBOW predicts a target word given its context, Skip-Gram predicts context words given a target word. The choice between CBOW and Skip-Gram depends on the specific characteristics of the data and the task at hand. Skip-Gram is known for performing better in scenarios where capturing fine-grained semantic relationships and word order is essential."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 9.\tExplain Glove Embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS :GloVe, which stands for Global Vectors for Word Representation, is another popular word embedding model used in natural language processing (NLP). Developed by researchers at Stanford University, GloVe is designed to capture global word-word co-occurrence statistics in a corpus and generate meaningful word embeddings. The key idea behind GloVe is to leverage the statistical information of word co-occurrences to represent words in a continuous vector space.\n",
    "\n",
    "Here's a summary of how GloVe works:\n",
    "\n",
    "1. **Co-occurrence Matrix:**\n",
    "   - GloVe starts by constructing a word co-occurrence matrix based on the entire corpus. The matrix \\(X\\) is constructed such that \\(X_{ij}\\) represents how often word \\(i\\) co-occurs with word \\(j\\).\n",
    "\n",
    "2. **Objective Function:**\n",
    "   - The core of GloVe lies in its objective function, which aims to learn word embeddings such that the dot product of these embeddings reflects the log-likelihood of the words' co-occurrence probabilities.\n",
    "   - The objective function includes both the observed and expected word co-occurrence probabilities.\n",
    "\n",
    "3. **Training:**\n",
    "   - GloVe is trained by minimizing the difference between the observed co-occurrence probabilities and the probabilities predicted by the model.\n",
    "   - This is achieved through iterative optimization, adjusting the word embeddings to improve the model's predictions.\n",
    "\n",
    "4. **Word Embeddings:**\n",
    "   - Once trained, the word embeddings obtained from GloVe represent words in a continuous vector space where the distances between vectors capture semantic relationships.\n",
    "   - The embeddings tend to capture not only syntactic relationships but also semantic relationships based on the global context of word co-occurrences in the corpus.\n",
    "\n",
    "5. **Usage and Advantages:**\n",
    "   - GloVe embeddings are pre-trained on large corpora and can be used for downstream NLP tasks without the need for task-specific training.\n",
    "   - These embeddings are known for capturing rich semantic information and are useful in various NLP applications such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "One advantage of GloVe is its ability to capture global context information, making it particularly useful for tasks that involve understanding the overall semantics of a document or a set of documents. Additionally, GloVe embeddings often exhibit good performance with smaller amounts of training data compared to other methods.\n",
    "\n",
    "In summary, GloVe is a powerful word embedding model that leverages global word co-occurrence statistics to generate meaningful and contextually rich word representations in a continuous vector space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
